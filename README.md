# EEG-to-Voice: Speech Synthesis from Brain Activity Recordings

## Overview

This project aims to develop a speech synthesis system from brain activity recordings using EEG data, particularly for individuals suffering from speech impairments due to neurodegenerative diseases such as amyotrophic lateral sclerosis (ALS), brain injuries, or cerebral palsy. The ultimate goal is to restore the ability to communicate verbally for patients who have lost this function by using Brain-Computer Interfaces (BCIs).

This research leverages variational autoencoding (VAE) to perform speech synthesis from EEG signals recorded from epileptic patients implanted with deep electrodes during language production tasks. By learning the correlation between brain activity and speech, this project aims to provide a natural and effective communication solution for individuals with speech disabilities, ultimately helping those in locked-in syndrome and similar conditions.

## Repository Structure

The project consists of several key scripts and directories:

### Folders
- **`cancor/`**  
  Contains the script `cca_analysis.py`, which performs canonical correlation analysis on the original audio and iEEG data for each patient. The results are saved in this directory, with each patientâ€™s result being stored as an image file (`acc_XXX.eps`), where `XXX` refers to the patient ID.
  
- **`iBDS-dataset/`**  
  Stores i-BDS formatted data for each patient, including audio and iEEG recordings after segmentation. The folder is organized as follows:
  - `audio_data/`: Raw audio data from patients.
  - `ieeg_data/`: Raw iEEG recordings from patients.
  - `audio_features/`: Extracted audio features.
  - `ieeg_features/`: Extracted iEEG features.
  - `validation/`: Contains test files evaluated by cross-validation folds for each patient.
  
- **`metrics/`**  
  Stores performance metrics for the models, calculated for each patient, validation fold, and test record. Generated by the `compute_audio_metrics.py` script as JSON files.

- **`transience/`**  
  Contains auxiliary functions used throughout the project.

### Key Scripts
- **`compute_audio_metrics.py`**  
  Evaluates performance metrics for the test samples by patient and fold, generating a JSON file that records results for each metric. These results are stored in the `metrics/` folder.

- **`extract_save_features.py`**  
  Extracts audio and iEEG features from the segmented recordings and saves them in the appropriate subfolders within `iBDS-dataset/`.

- **`plot_evaluation_metrics.py`**  
  Plots the validation results from the data in the JSON file generated by `compute_audio_metrics.py`, enabling the visualization of model performance.

- **`s2a_feature_synthesis.py`**  
  Implements the design, training, validation, and testing of the speech synthesis models. This script is the core of the project, focusing on transforming brain activity into speech.

- **`setup.cfg`**  
  Configuration file containing hyperparameters for feature extraction, model design, and training.

### Other Files
- **`requirements-windows.txt`**  
  Contains the required Python dependencies for running the project on a Windows system. Install them using the following command:
  ```bash
  pip install -r requirements-windows.txt
  ```

- **`requirements-wsl2.txt`**  
  Contains the required dependencies for running on Linux systems or Windows Subsystem for Linux 2 (WSL 2). Some scripts, such as `compute_audio_metrics.py`, may not be supported on Windows due to package compatibility. Install these dependencies using:
  ```bash
  pip install -r requirements-wsl2.txt
  ```

## Setup and Installation

1. **Windows Installation**  
   Install the required dependencies:
   ```bash
   pip install -r requirements-windows.txt
   ```

2. **Linux Installation or WSL 2 (Recommended for Full Functionality)**  
   Some scripts require Linux-based environments for full compatibility, particularly `compute_audio_metrics.py`, due to unsupported packages on Windows. We recommend using **Windows Subsystem for Linux 2 (WSL 2)** if a Linux machine is unavailable. To install the required dependencies, run:
   ```bash
   pip install -r requirements-linux.txt
   ```

## Usage Instructions

1. **Feature Extraction**  
   Extract audio and iEEG features by running:
   ```bash
   python extract_save_features.py
   ```

2. **Training and Testing the Speech Synthesis Model**  
   Design, train, and test the models by executing:
   ```bash
   python s2a_feature_synthesis.py
   ```

3. **Evaluating Model Performance**  
   Compute performance metrics using:
   ```bash
   python compute_audio_metrics.py
   ```
   This script will generate a JSON file with the evaluation results.

4. **Plotting Evaluation Results**  
   Visualize the results by running:
   ```bash
   python plot_evaluation_metrics.py
   ```

## Contributions and Future Work

This project lays the foundation for non-verbal communication systems using brain activity recordings. In the future, the system could be enhanced by:
- Extending the dataset with more patients and diverse tasks.
- Improving the synthesis model architecture to produce more natural speech.
- Exploring alternative BCI modalities to enhance accuracy and usability.

## License

This project is licensed under the MIT License.